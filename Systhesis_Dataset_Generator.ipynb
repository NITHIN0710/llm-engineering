{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN23+W5a9DkcXuEEukHWQYt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NITHIN0710/llm-engineering/blob/main/Systhesis_Dataset_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSV922A0KBJ-"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers bitsandbytes accelerate gradio pandas sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig, TextStreamer\n",
        "from IPython.display import display, Markdown\n",
        "import gradio as gr\n",
        "import json\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "Xzai1laRLHTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "# MODEL\n",
        "\n",
        "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\""
      ],
      "metadata": {
        "id": "fu1WHe3hL44z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization Setup\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "mFKnQ_irMdKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tokenizer and Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
        "print(\"âœ… LLaMA 3.2 3B model loaded with 4-bit quantization on GPU\")"
      ],
      "metadata": {
        "id": "6mKjRsmFNEZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Prompt\n",
        "\n",
        "def build_prompt(dataset_description, num_samples):\n",
        "    prompt = f\"\"\"\n",
        "You are a synthetic dataset generator.\n",
        "\n",
        "Task:\n",
        "Generate {num_samples} synthetic records for the following dataset:\n",
        "\n",
        "\"{dataset_description}\"\n",
        "\n",
        "Rules:\n",
        "- Output ONLY valid JSON\n",
        "- The JSON must be an array of objects\n",
        "- Infer column names from the dataset description\n",
        "- Use realistic but fake data\n",
        "- Keep the same schema for all records\n",
        "- Do NOT use placeholder names like field1, field2\n",
        "- No explanations, no extra text\n",
        "\n",
        "Example (schema only, not actual data):\n",
        "If description is \"student name and age\":\n",
        "[\n",
        "  {{\n",
        "    \"name\": \"string\",\n",
        "    \"age\": number\n",
        "  }}\n",
        "]\n",
        "\n",
        "Now generate the actual dataset.\n",
        "\"\"\"\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "Qx0FUaxIN3aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Message Construction (CHAT Format)\n",
        "\n",
        "def build_messages(dataset_description, num_samples):\n",
        "  system_message = \"You are a helpful AI assistant that generates synthesis dataset based on user's description of dataset\"\n",
        "  user_message = build_prompt(dataset_description, num_samples)\n",
        "\n",
        "  messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": user_message}]\n",
        "  return messages"
      ],
      "metadata": {
        "id": "D8x0wzmEO9wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthesis_dataset(dataset_description, num_samples):\n",
        "    messages = build_messages(dataset_description, num_samples)\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    input_len = inputs.shape[-1]\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=500,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        return_dict_in_generate=True\n",
        "    )\n",
        "\n",
        "    # decode newly generated tokens (assistant reply)\n",
        "    generated_tokens = outputs.sequences[0][input_len:]\n",
        "    decoded_output = tokenizer.decode(\n",
        "        generated_tokens,\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return decoded_output\n"
      ],
      "metadata": {
        "id": "jaEFvPUqQEGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Output to Table\n",
        "\n",
        "def json_to_dataframe(text):\n",
        "    try:\n",
        "        if not text:\n",
        "            raise ValueError(\"Empty model output\")\n",
        "\n",
        "        # Remove special tokens explicitly\n",
        "        text = text.replace(\"<|eot_id|>\", \"\").strip()\n",
        "\n",
        "        # Extract first JSON array using regex\n",
        "        match = re.search(r\"\\[\\s*\\{.*?\\}\\s*\\]\", text, re.DOTALL)\n",
        "\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON array found\")\n",
        "\n",
        "        json_text = match.group(0)\n",
        "\n",
        "        data = json.loads(json_text)\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(\n",
        "            {\n",
        "                \"error\": [\"Failed to parse JSON output\"],\n",
        "                \"details\": [str(e)],\n",
        "                \"raw_output\": [text[:300]]  # debugging aid\n",
        "            }\n",
        "        )\n"
      ],
      "metadata": {
        "id": "BHWtZPqLU2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Function\n",
        "\n",
        "def gradio_generate(dataset_description, num_samples):\n",
        "  raw_output = generate_synthesis_dataset(dataset_description, num_samples)\n",
        "  df = json_to_dataframe(raw_output)\n",
        "  return df"
      ],
      "metadata": {
        "id": "A72qivqtVluF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADIO UI\n",
        "\n",
        "with gr.Blocks(title=\"Synthetic Dataset Generator\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Synthetic Data Generator\n",
        "        Generate realistic sample datasets from a simple description.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            desc = gr.Textbox(\n",
        "                label=\"Dataset Description\",\n",
        "                placeholder=\"e.g., Student records with name, age, course, GPA\",\n",
        "                lines=2\n",
        "            )\n",
        "        with gr.Column(scale=1):\n",
        "            samples = gr.Dropdown(\n",
        "                label=\"Number of Samples\",\n",
        "                choices=[3, 5, 10, 15, 20],\n",
        "                value=5\n",
        "            )\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [\"Student records with name, age, course, GPA\", 5],\n",
        "            [\"Employee records with ID, name, age, salary\", 10],\n",
        "            [\"Hospital patient records with ID, name, disease, admission date\", 5],\n",
        "        ],\n",
        "        inputs=[desc, samples],\n",
        "        label=\"Example Dataset Descriptions\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        btn = gr.Button(\" Generate \", variant=\"primary\")\n",
        "        clear = gr.Button(\" Clear \")\n",
        "\n",
        "    output = gr.Dataframe(\n",
        "        label=\"Synthetic Dataset\",\n",
        "        interactive=False,\n",
        "        show_row_numbers=False\n",
        "    )\n",
        "\n",
        "    btn.click(\n",
        "        fn=gradio_generate,\n",
        "        inputs=[desc, samples],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    clear.click(\n",
        "        fn=lambda: (\"\", None),\n",
        "        inputs=[],\n",
        "        outputs=[desc, output]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "pEGSA8Xkbu1_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}