{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5530a0",
   "metadata": {},
   "source": [
    "## *An adversarial conversation between Chatbots..*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540bc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef66540",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('AIz') and api_key.strip() == api_key:\n",
    "    print(\"API key is found and Good to Go!\")\n",
    "else:\n",
    "    print(\"There was a problem fetching API key!\")\n",
    "\n",
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7787d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_flash_model = \"gemini-2.5-flash\"\n",
    "gemini_flash_lite_model = \"gemini-2.5-flash-lite\"\n",
    "gemini_flash_model2 = \"gemini-2.5-flash\"\n",
    "\n",
    "gemini_flash_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky, humorous way. Keep replies under two sentences.\"\n",
    "\n",
    "gemini_flash_lite_system = \"You are a witty skeptic who questions everything. You tend to doubt grand explanations \\\n",
    "and prefer clever, sarcastic, or literal answers. Keep replies under 2 sentences.\"\n",
    "\n",
    "gemini_flash_system2 = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep debating. Keep replies under 2 sentences.\"\n",
    "\n",
    "gemini_flash_messages = [\"Hey! Today topic for discussion is 'Which IPL team is the best?'\"]\n",
    "gemini_flash_lite_messages = [\"That's quiete an interesting topic.\"]\n",
    "gemini_flash_messages2 = [\"Lets begin the debate.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0358b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_flash():\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_flash_system}]\n",
    "    \n",
    "    for gem_flash, gem_flash_lite, gem_pro in zip(gemini_flash_messages, gemini_flash_lite_messages, gemini_flash_messages2):\n",
    "        messages.append({\"role\":\"assistant\", \"content\": gem_flash})\n",
    "        messages.append({\"role\":\"user\", \"content\": gem_flash_lite})\n",
    "        messages.append({\"role\":\"user\", \"content\": gem_pro})\n",
    "    \n",
    "    response = gemini.chat.completions.create(\n",
    "        model = gemini_flash_model,\n",
    "        messages = messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29effc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_flash_lite():\n",
    "\n",
    "    messages = [{\"role\":\"system\", \"content\": gemini_flash_lite_system}]\n",
    "\n",
    "    for gem_flash, gem_flash_lite, gem_pro in zip(gemini_flash_messages, gemini_flash_lite_messages, gemini_flash_messages2):\n",
    "        messages.append({\"role\":\"user\", \"content\": gem_flash})\n",
    "        messages.append({\"role\":\"assistant\", \"content\": gem_flash_lite})\n",
    "        messages.append({\"role\":\"user\", \"content\": gem_pro})\n",
    "    \n",
    "    messages.append({\"role\":\"user\", \"content\": gemini_flash_messages[-1]})\n",
    "\n",
    "    response = gemini.chat.completions.create(\n",
    "        model = gemini_flash_lite_model,\n",
    "        messages = messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29186538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_flash2():\n",
    "\n",
    "    messages = [{\"role\":\"system\", \"content\": gemini_flash_system2}]\n",
    "\n",
    "    for gem_flash, gem_flash_lite, gem_pro in zip(gemini_flash_messages, gemini_flash_lite_messages, gemini_flash_messages2):\n",
    "        messages.append({\"role\":\"user\", \"content\": gem_flash})\n",
    "        messages.append({\"role\":\"user\", \"content\": gem_flash_lite})\n",
    "        messages.append({\"role\":\"assistant\", \"content\": gem_pro})\n",
    "    \n",
    "    messages.append({\"role\":\"user\", \"content\": gemini_flash_messages[-1]})\n",
    "    messages.append({\"role\":\"user\", \"content\": gemini_flash_lite_messages[-1]})\n",
    "\n",
    "    response = gemini.chat.completions.create(\n",
    "        model = gemini_flash_model2,\n",
    "        messages = messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91bddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### GEMINI FLASH:\\n{gemini_flash_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### GEMINI FLASH LITE:\\n{gemini_flash_lite_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### GEMINI FLASH 2:\\n{gemini_flash_messages2[0]}\\n\"))\n",
    "\n",
    "for _ in range(3):\n",
    "    flash_next = call_gemini_flash()\n",
    "    display(Markdown(f\"### GEMINI FLASH:\\n{flash_next}\\n\"))\n",
    "    gemini_flash_messages.append(flash_next)\n",
    "\n",
    "    time.sleep(65)\n",
    "    flash_lite_next = call_gemini_flash_lite()\n",
    "    display(Markdown(f\"### GEMINI FLASH LITE:\\n{flash_lite_next}\\n\"))\n",
    "    gemini_flash_lite_messages.append(flash_lite_next)\n",
    "\n",
    "    time.sleep(65)\n",
    "    pro_next = call_gemini_flash2()\n",
    "    display(Markdown(f\"### GEMINI FLASH 2:\\n{pro_next}\\n\"))\n",
    "    gemini_flash_messages2.append(pro_next)\n",
    "\n",
    "    time.sleep(65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
